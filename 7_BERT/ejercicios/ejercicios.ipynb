{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/institutohumai/cursos-python/blob/master/NLP/7_BERT/ejercicios/ejercicios.ipynb\"> <img src='https://colab.research.google.com/assets/colab-badge.svg' /> </a>"
      ],
      "metadata": {
        "id": "eBZFa6AWPI1s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicios Clase 7"
      ],
      "metadata": {
        "id": "it7rwdsHP1Ur"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhAqZLs3lwhW",
        "cellView": "form"
      },
      "source": [
        "# @markdown Setup\n",
        "# Fist install the library and download the models from github\n",
        "\n",
        "!pip install transformers\n",
        "!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/pytorch_weights.tar.gz \n",
        "!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/vocab.txt \n",
        "!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/config.json \n",
        "!tar -xzvf pytorch_weights.tar.gz\n",
        "!mv config.json pytorch/.\n",
        "!mv vocab.txt pytorch/.\n",
        "\n",
        "# import the necessary\n",
        "\n",
        "import torch\n",
        "from transformers import BertForMaskedLM, BertTokenizer\n",
        "\n",
        "# create the tokenizer and the model\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"pytorch/\", do_lower_case=False)\n",
        "model = BertForMaskedLM.from_pretrained(\"pytorch/\")\n",
        "e = model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 1\n",
        "\n",
        "A partir de la versión de Beto descargada en las celdas de código anteriores, implementar un pequeño pipeline que prediga la siguiente palabra.\n",
        "\n",
        "El pipeline debera: \n",
        "1. Tomar el texto inicial y convertirlo a índices enteros\n",
        "2. Agregar al final del texto inicial el token correspondiente a máscara `\"[MASK]\"`\n",
        "3. Alimentar al modelo con el texto con máscara\n",
        "4. Reemplazar `\"[MASK]\"` con la predicción hecha por el modelo\n",
        "5. Repetir el proceso unas 6 o 7 veces"
      ],
      "metadata": {
        "id": "2yJEHGeRBpxj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb9CZLSgQphw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c98f41d9-9fb9-4753-fd50-7ed490e84ab5"
      },
      "source": [
        "text = \"[CLS] Era otro día de verano,\"\n",
        "mask_tkn = tokenizer.convert_tokens_to_ids(\"[MASK]\")\n",
        "sep_tkn = tokenizer.convert_tokens_to_ids(\"[SEP]\")\n",
        "\n",
        "\n",
        "tokens = tokenizer.tokenize(text)\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
        "for i in range(20):\n",
        "    indexed_tokens.append(mask_tkn)\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    predictions = model(tokens_tensor)[0]\n",
        "    idxs = torch.argsort(predictions[0,-1], descending=True)\n",
        "    new_tkn = int(idxs[0])\n",
        "    indexed_tokens[-1] = new_tkn\n",
        "    if new_tkn == sep_tkn:\n",
        "      break\n",
        "\n",
        "print(' '.join(tokenizer.convert_ids_to_tokens(indexed_tokens)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] Era otro día de verano , y yo estaba soñando despierto , soñando despierto , soñando despierto , soñando despierto , soñando despierto , soñando despierto\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 2\n",
        "\n",
        "Implemente un pequeño pipepile que genere un beam search. No hace falta que su implemntación no sea óptima en términos de eficiencia temporal. \n",
        "\n",
        "Para esto debera crear:\n",
        "\n",
        "Función `children(model, beam, indeces)`:\n",
        "  * Recibe el modelo `model` a partir del cual generar predicciones.\n",
        "  * Recibe el tamaño del haz `beam`\n",
        "  * Recibe un conjunto de índices `indeces` correspondiente la palabra original\n",
        "\n",
        "La función `children` deberá:\n",
        "  1. Agregar al final del texto inicial el token correspondiente a máscara `\"[MASK]\"`\n",
        "  3. Alimentar al modelo con el texto con máscara\n",
        "  5. A la salida del modelo BERT tenemos una serie de números que no son probabilidades. Para convertirla a probabilidades debera usar `torch.softmax`\n",
        "  3. Ordenar las predicciones de la más probable a la menos probable\n",
        "      * revise la documentación de la función `torch.sort` y comparela con `torch.argsort`\n",
        "  4. Tomar las primeras predicciones usando `beam`. Si `beam = 5` tomara las 5 primeras predicciones. \n",
        "  4. Debera guardar como puntaje el logaritmo de las probabilides de las primeras predicciones.\n",
        "  5. La función debe devolver un `zip` del indice predicho y su puntaje.\n",
        "\n",
        "Función `new_top(model, beam, top, scores)`:\n",
        "  * Recibe el modelo `model` a partir del cual generar predicciones.\n",
        "  * Recibe el tamaño del haz `beam`\n",
        "  * Recibe un conjunto de las mejores predicciones `top`\n",
        "  * Recibe el puntaje `scores` de cada una de las predicciones en `top`\n",
        "\n",
        "La función `new_top` deberá:\n",
        "  1. Iterar sobre cada elemento de `top` y `scores` usando un zip.\n",
        "  1. Iterar sobre cada elemento devuelto de la función `children` usando como entrada el puntaje del iterador anterior\n",
        "  2. A partir de la prediccion y de el puntaje de la función `children` actualizar los índices de `top` y el `score`\n",
        "  3. Guardar todos los indices acutalizados con su puntaje. Es conveniente agruparlos en una misma estructura para luego ordenalos de mayor a menos\n",
        "    * debería tener `beam * beam` candidatos\n",
        "  4. Ordenar los candidatos de mayor a menor y conservar los primeros. Si `beam = 5` tomara las 5 primeras predicciones. \n",
        "  5. Devuelva los nuevos top y scores\n",
        "\n",
        "Con estas funciones, solo deberá llamar a la la función `children` una única vez para genrar 10 candidatos y luego hacer iteracioens con la función `new_top` para implementar un beam search\n",
        "\n",
        "> Recuerde mostrar algunos de los resultado obtenidos\n",
        "\n",
        "Este proceso no da muy buenos resultados. ¿Por que lo cree?\n"
      ],
      "metadata": {
        "id": "JaMXIJhYCAcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"[CLS] Ese día, desde mi ventana\"\n",
        "beam = 10\n",
        "\n",
        "def children(model, beam, indeces):\n",
        "    mask_tkn = tokenizer.convert_tokens_to_ids(\"[MASK]\")\n",
        "    indeces.append(mask_tkn)\n",
        "    tokens_tensor = torch.tensor([indeces])\n",
        "    predictions = model(tokens_tensor)[0]\n",
        "    new_score, idxs = torch.sort(predictions[0,-1], descending=True)\n",
        "    new_score = torch.softmax(new_score,0)\n",
        "    new_score, idxs = new_score[:beam], idxs[:beam]\n",
        "    new_score = torch.log(new_score)\n",
        "    return zip(new_score, idxs)\n",
        "\n",
        "def new_top(model, beam, top, scores):\n",
        "    sep_tkn = tokenizer.convert_tokens_to_ids(\"[SEP]\")\n",
        "    eos_tkn = tokenizer.convert_tokens_to_ids(\"[EOS]\")\n",
        "    stop_tkn = tokenizer.convert_tokens_to_ids(\".\")\n",
        "    candidates = []\n",
        "    for value,indeces in zip(scores, top):\n",
        "        if value == float('inf'):\n",
        "            candidates.append((value, indeces))\n",
        "            continue\n",
        "        for new_val, new_idx in children(model, beam, indeces):\n",
        "            new_indeces = indeces.copy()\n",
        "            new_indeces[-1] = int(new_idx)\n",
        "            if new_idx == sep_tkn or new_idx == eos_tkn:\n",
        "                new_val = float('inf')\n",
        "            if new_idx == stop_tkn:\n",
        "                new_indeces.append(sep_tkn)\n",
        "            #\"\"\"\n",
        "            #for i in range((len(new_indeces) +1 )//2 ):\n",
        "            for i in range(4):\n",
        "                penalty = 0\n",
        "                if new_indeces[-1] == new_indeces[- 2 - i]:\n",
        "                    penalty = -1000\n",
        "                    for j in range(1,i + 1):\n",
        "                        if new_indeces[-1 - j] != new_indeces[- 2 - i - j]:\n",
        "                            penalty = 0\n",
        "                            break\n",
        "                    value += penalty\n",
        "            #\"\"\"\n",
        "            candidates.append((value + float(new_val), new_indeces))\n",
        "    candidates = sorted(candidates, key = lambda x: x[0], reverse=True)\n",
        "    candidates = candidates[:beam * beam]\n",
        "    return zip(*candidates)\n",
        "\n",
        "tokens = tokenizer.tokenize(text)\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input = [indexed_tokens.copy()]\n",
        "scores, top = new_top(model, beam, input,[0])\n",
        "for i in range(5):\n",
        "    scores, top = new_top(model, beam, top, scores)\n",
        "    #scores, top = scores[:beam], top[:beam]\n",
        "scores, top = scores[:20], top[:20]\n",
        "for value,indeces in zip(scores, top):\n",
        "    print()\n",
        "    print(' '.join(tokenizer.convert_ids_to_tokens(indeces)), \"//\",value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "239M1dp1LljJ",
        "outputId": "28b187b8-69e4-4641-f36c-b6998c7ca9f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[CLS] Ese día , desde mi ventana , ese día , desde mi // -4.901955099776387\n",
            "\n",
            "[CLS] Ese día , desde mi ventana , al día siguiente , desde // -7.438374161720276\n",
            "\n",
            "[CLS] Ese día , desde mi ventana , ese día , desde la // -7.685020821169019\n",
            "\n",
            "[CLS] Ese día , desde mi ventana , vi lo que vi y // -7.965865343809128\n",
            "\n",
            "[CLS] Ese día , desde mi ventana , vi a W ##hite W // -7.978757441043854\n",
            "\n",
            "[CLS] Ese día , desde mi ventana , supe lo que era lo // -8.059721946716309\n",
            "\n",
            "[CLS] Ese día , desde mi ventana , tú y yo , juntos // -8.088517896831036\n",
            "\n",
            "[CLS] Ese día , desde mi ventana , vi al hombre que me // -8.27081111073494\n",
            "\n",
            "[CLS] Ese día , desde mi ventana , vi lo que vi , // -8.374415546655655\n",
            "\n",
            "[CLS] Ese día , desde mi ventana , ese día , yo , // -8.453999923542142\n",
            "\n",
            "[CLS] Ese día , desde mi ventana , lo que vi fue lo // -8.624116629362106\n",
            "\n",
            "[CLS] Ese día , desde mi ventana , lo que vi , fue // -8.68254765868187\n",
            "\n",
            "[CLS] Ese día , desde mi ventana , supe lo que vi , // -8.694658041000366\n",
            "\n",
            "[CLS] Ese día , desde mi ventana , ese día , desde tu // -8.749931948259473\n",
            "\n",
            "[CLS] Ese día , desde mi ventana , vi lo que era lo // -8.751679807901382\n",
            "\n",
            "[CLS] Ese día , desde mi ventana , supe lo que vi y // -8.769010305404663\n",
            "\n",
            "[CLS] Ese día , desde mi ventana , lo supe todo , todo // -8.774520248174667\n",
            "\n",
            "[CLS] Ese día , desde mi ventana , lo que fue lo que // -8.776463707908988\n",
            "\n",
            "[CLS] Ese día , desde mi ventana , supe lo que era ser // -8.802815437316895\n",
            "\n",
            "[CLS] Ese día , desde mi ventana , ese día , desde mis // -8.867653267458081\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 3\n",
        "\n",
        "Tomando de referencia el siguiente código busque un modelo en español de huggingface preentrenado y pruebe la tarea en la que fue preentrenado.\n",
        "\n",
        "El código a continuación es PoS (Part of Speech) para tipo de palabra, pero usted puede elegir otra tarea al a que se le ha aplicado fine tuning para un modelo de BERT.\n",
        "\n"
      ],
      "metadata": {
        "id": "rdu5KoVwLinp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "nlp_ner = pipeline(\n",
        "    \"ner\",\n",
        "    model=\"mrm8488/RuPERTa-base-finetuned-pos\",\n",
        "    tokenizer=\"mrm8488/RuPERTa-base-finetuned-pos\")\n",
        "\n",
        "text = 'tras el temporal, el joven encontro un empleo temporal'\n",
        "text = 'Hacia el este fue donde este sujeto se dirigio y se topo con este'\n",
        "\n",
        "\n",
        "outputs = nlp_ner(text)\n",
        "\n",
        "for i in outputs:\n",
        "  print(i['word'], i['entity'])"
      ],
      "metadata": {
        "id": "EKkn4qC5_95P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "nlp_qa = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=\"inigopm/beto-base-spanish-squades2\",\n",
        "    tokenizer=\"inigopm/beto-base-spanish-squades2\")\n",
        "\n",
        "example = {'context' : 'Para preparar un mate cebado, se coloca la yerba en un recipiente llamado mate o calabaza, hasta las tres cuartas partes del mismo. Luego se tapa con la mano, se coloca boca abajo y se lo agita (esto hace que las partículas más finas queden en la parte superior, y no obstruyan la bombilla). Se lo coloca nuevamente boca arriba y se le agrega un poco de agua tibia o fría cerca del borde. Se deja reposar algunos segundos (hasta que se absorba el agua) y se termina de llenar con agua caliente, hasta aproximadamente 7 u 8 mm del borde, cuidando de que no se moje la yerba de la superficie (el agua caliente debe estar a una temperatura cercana a 80 °C, antes del punto de ebullición). Luego de uno o dos minutos se ensilla, es decir, se coloca la bombilla tapándole la boca con el dedo pulgar y presionando firmemente hasta el fondo. Siempre tiene que quedar más yerba sobre el lado opuesto de la bombilla. Se debe tener cuidado de no remover demasiado la yerba, porque podría taparse la bombilla. La bombilla se debe inclinar en sentido contrario a donde quedó la yerba seca, es decir, para el lado del que va a tomar el mate. ',\n",
        " 'question' :'¿Dónde se coloca la yerba para preparar un mate cebado?'}\n",
        "\n",
        "example = {'context' : 'Para preparar un mate cebado, se coloca la yerba en un recipiente llamado mate o calabaza, hasta las tres cuartas partes del mismo. Luego se tapa con la mano, se coloca boca abajo y se lo agita (esto hace que las partículas más finas queden en la parte superior, y no obstruyan la bombilla). Se lo coloca nuevamente boca arriba y se le agrega un poco de agua tibia o fría cerca del borde. Se deja reposar algunos segundos (hasta que se absorba el agua) y se termina de llenar con agua caliente, hasta aproximadamente 7 u 8 mm del borde, cuidando de que no se moje la yerba de la superficie (el agua caliente debe estar a una temperatura cercana a 80 °C, antes del punto de ebullición). Luego de uno o dos minutos se ensilla, es decir, se coloca la bombilla tapándole la boca con el dedo pulgar y presionando firmemente hasta el fondo. Siempre tiene que quedar más yerba sobre el lado opuesto de la bombilla. Se debe tener cuidado de no remover demasiado la yerba, porque podría taparse la bombilla. La bombilla se debe inclinar en sentido contrario a donde quedó la yerba seca, es decir, para el lado del que va a tomar el mate. ',\n",
        " 'question' :'¿Hasta dónde se llena el mate para preparar un mate cebado?'}\n",
        "\n",
        "\n",
        "outputs = nlp_qa(example)\n",
        "\n",
        "outputs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cp7Aq440MKcW",
        "outputId": "07a8dbe5-afc2-4efe-ec82-34326e403185"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.0909624993801117,\n",
              " 'start': 101,\n",
              " 'end': 120,\n",
              " 'answer': 'tres cuartas partes'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}